{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "description": "Metaschema for validating the structure of the per-doctype schemas in this repository",
  "id": "http://jsonschema.net",
  "properties": {
    "mozPipelineMetadata": {
      "additionalProperties": false,
      "description": "Container for per-doctype metadata that can affect how pings are processed in the pipeline",
      "properties": {
        "$comment": {
          "description": "Optional comment about the pipeline handling for this doctype",
          "type": "string"
        },
        "bq_dataset_family": {
          "description": "NOT YET IMPLEMENTED: The base name for the destination BigQuery dataset; if this is set to 'telemetry', the pipeline will write into 'telemetry_live'",
          "type": "string"
        },
        "bq_metadata_format": {
          "description": "The logical format for the metadata struct in the destination BigQuery table",
          "enum": [
            "structured",
            "telemetry",
            "pioneer"
          ],
          "type": "string"
        },
        "bq_table": {
          "description": "NOT YET IMPLEMENTED: The name of the destination BigQuery table",
          "type": "string"
        },
        "expiration_policy": {
          "additionalProperties": false,
          "description": "Various options controlling data lifecycle",
          "properties": {
            "collect_through_date": {
              "description": "If present, the pipeline will reject new data with submission_timestamp after the given date, sending it to error output",
              "pattern": "^[0-9]{4}-[0-9]{2}-[0-9]{2}$",
              "type": "string"
            },
            "delete_after_days": {
              "description": "If present, a time_partitioning_expiration policy will be set on the destination stable table in BigQuery",
              "type": "integer"
            }
          },
          "type": "object"
        },
        "jwe_mappings": {
          "description": "Mappings of encrypted JWE field paths to destinations where the value decrypted by the pipeline should be placed; initial use case is Account Ecosystem Telemetry; paths must be in [JSON Pointer format](https://tools.ietf.org/html/rfc6901) like '/payload/ecosystemAnonId'",
          "items": {
            "additionalProperties": false,
            "properties": {
              "decrypted_field_path": {
                "pattern": "^/.*$",
                "type": "string"
              },
              "source_field_path": {
                "pattern": "^/.*$",
                "type": "string"
              }
            },
            "required": [
              "source_field_path",
              "decrypted_field_path"
            ],
            "type": "object"
          },
          "type": "array"
        },
        "override_attributes": {
          "description": "Mappings of Pub/Sub attribute names to static values; these are applied in the Decoder immediately before incorporating metadata into the payload, so can be used to overwrite values calculated in the pipeline; a null value will cause the pipeline to drop the named attribute; some attribute names differ from the nested metadata format in BigQuery, so for example you must use \"geo_city\" here in order to manipulate the value that shows up as metadata.geo.city; implemented for bug 1742172",
          "items": {
            "additionalProperties": false,
            "properties": {
              "name": {
                "enum": [
                  "geo_city",
                  "geo_subdivision1",
                  "geo_subdivision2",
                  "normalized_channel"
                ],
                "type": "string"
              },
              "value": {
                "type": [
                  "string",
                  "null"
                ]
              }
            },
            "required": [
              "name",
              "value"
            ],
            "type": "object"
          },
          "type": "array"
        },
        "submission_timestamp_granularity": {
          "description": "If specified, the submission_timestamp field will be truncated to the specified granularity in the pipeline before being output to BigQuery; this can be used to reduce the potential for using time-based attacks to correlate datasets using different client-level identifiers; see Java's ChronoUnit for additional granularities that could be considered for inclusion; implemented for bug 1742172",
          "enum": [
            "millis",
            "seconds",
            "minutes",
            "hours",
            "days"
          ],
          "type": "string"
        }
      },
      "type": "object"
    }
  },
  "type": "object"
}
